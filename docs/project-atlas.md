# Modal llama-cpp Server - Proje AtlasÄ±

## Proje AmacÄ±

Modal llama-cpp Server, serverless GPU altyapÄ±sÄ± Ã¼zerinde Ã§alÄ±ÅŸan, OpenAI uyumlu GGUF model API servisidir. KullanÄ±cÄ±larÄ±n herhangi bir GPU konfigÃ¼rasyonu yapmadan, otomatik olarak optimize edilmiÅŸ ÅŸekilde GGUF quantized modellerini servis etmelerini saÄŸlar.

## Temel Felsefe

- **GGUF Optimizasyonu**: Q8_0 quantization ile optimal performans
- **Kolay KullanÄ±m**: Tek komutla GGUF model servisi baÅŸlatma
- **Maliyet EtkinliÄŸi**: Modal'Ä±n serverless yapÄ±sÄ± ile sadece kullanÄ±ldÄ±ÄŸÄ±nda Ã¶deme
- **Uyumluluk**: OpenAI API standardÄ±na tam uyum
- **HÄ±zlÄ± BaÅŸlatma**: vLLM'den 3x daha hÄ±zlÄ± startup sÃ¼resi

## Mimari Genel BakÄ±ÅŸ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ‘¤ KullanÄ±cÄ±    â”‚â”€â”€â”€â–¶â”‚ ğŸ¯ GPU SeÃ§imi    â”‚â”€â”€â”€â–¶â”‚ ğŸ¦™ llama-cpp  â”‚
â”‚ Ä°steÄŸi          â”‚    â”‚   AlgoritmasÄ±     â”‚    â”‚   Server      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                        â”‚
                              â–¼                        â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ ğŸ“¥ GGUF Model    â”‚    â”‚ ğŸ”Œ OpenAI API â”‚
                       â”‚ YÃ¶netimi         â”‚    â”‚ Endpoint      â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Teknoloji Stack

- **Modal**: Serverless GPU infrastructure
- **llama-cpp-python**: High-performance GGUF inference engine
- **FastAPI**: Modern web framework (llama-cpp iÃ§inde)
- **Hugging Face**: Model repository ve tokenizer
- **CUDA**: GPU acceleration with CUBLAS

## Desteklenen Model FormatlarÄ±

- **SafeTensors**: GÃ¼venli tensor formatÄ± (Ã¶nerilen)
- **PyTorch**: Geleneksel .bin formatÄ±
- **GGUF**: Quantized model formatÄ± (v0.10.0+) âœ… YENÄ°!
- **Quantization**: GPTQ, AWQ, BitsAndBytes

### GGUF Format DesteÄŸi (Yeni Ã–zellik)
- **Tek dosyalÄ± yapÄ±**: TÃ¼m model tek .gguf dosyasÄ±nda
- **Built-in quantization**: Dahili sÄ±kÄ±ÅŸtÄ±rma
- **MoE model desteÄŸi**: Mixture of Experts modeller
- **Otomatik GPU seÃ§imi**: 18.4B MoE iÃ§in H100 seÃ§imi
- **Hedef model**: `DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF`

## GPU SeÃ§imi Stratejisi

| Model Boyutu | Quantized | GPU SeÃ§imi | Memory KullanÄ±mÄ± |
|-------------|-----------|------------|------------------|
| 70B+        | âŒ        | B200/H200  | 95% |
| 70B         | âœ…        | H100       | 85% |
| 30B-65B     | âŒ        | H100       | 85% |
| 13B-30B     | âŒ        | A100-80GB  | 80% |
| 7B-13B      | âŒ        | L40S       | 75% |
| 3B-7B       | Any       | A10G       | 75% |
| 1B-3B       | Any       | L4         | 70% |

## Temel Prensipler

1. **Memory Safety**: Konservatif memory allocation ile OOM hatalarÄ±nÄ± Ã¶nleme
2. **Auto-scaling**: Modal'Ä±n otomatik Ã¶lÃ§eklendirme Ã¶zelliklerini kullanma
3. **Persistent Storage**: Model cache iÃ§in volume kullanÄ±mÄ±
4. **Error Resilience**: KapsamlÄ± hata yÃ¶netimi ve fallback mekanizmalarÄ±
5. **Performance**: Model boyutuna gÃ¶re optimize edilmiÅŸ konfigÃ¼rasyonlar

## KullanÄ±m SenaryolarÄ±

- **GeliÅŸtirme**: HÄ±zlÄ± prototipleme ve test
- **ProdÃ¼ksiyon**: Ã–lÃ§eklenebilir API servisi
- **AraÅŸtÄ±rma**: FarklÄ± modelleri deneme
- **RP/Chat**: Roleplay ve sohbet uygulamalarÄ±
